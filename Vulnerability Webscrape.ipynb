{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages & libraries\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# iPortal Data\n",
    "\n",
    "# EXCEL IMPORT & EXPORT \n",
    "\n",
    "# imports data from excel based on location, drops unneeded columns\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "iPortal_import = pd.read_excel(file_path + 'iPortal.xlsx', sheet_name = 'Assets')\n",
    "iPortal_import = iPortal_import.drop(columns = ['Type', 'PLC', 'Rack', 'SubAddress', 'Slot', 'SubSlot', 'Module', 'Order number',\n",
    "                                           'GSDML files', 'Online-MLFB', 'Online-HW version', 'Online serial No.', 'IP',\n",
    "                                           'Subnet', 'Gateway', 'MAC', 'Device ID', 'Robot', 'Rob. Cell name', 'Rob. Controller', \n",
    "                                            'Rob. Serial.', 'Rob. KSS', 'ANZ. Ext. Axes', 'PN-stack active', 'ANZ. secure IOs', 'Number of IOs',\n",
    "                                           'Version of PROFINET', 'MasterBusCycleTime', 'MasterBusTimeout', 'SlaveBusCycleTime', \n",
    "                                           'SlaveBusTimeout', 'Profienergy enabled', 'ProfiEnergyTimeToPauseHibernate',\n",
    "                                           'ProfiEnergyTimeMinLengthOfStayHibernate', 'ProfiEnergyTimeToOperateHibernate',\n",
    "                                           'ProfiEnergyTimeToPauseDrivesOff', 'ProfiEnergyTimeMinLengthOfStayDrivesOff',\n",
    "                                           'ProfiEnergyTimeToOperateDrivesOff', 'PN product name', 'Active participants',\n",
    "                                            'User-ID','Cycle time', 'SubSystem', 'Version', 'comment', 'Manufacturer'])\n",
    "print(iPortal_import)\n",
    "\n",
    "# filling in empty/NaN cells w/ value preceding the cell \n",
    "iPortal_data = iPortal_import.fillna(method='ffill')\n",
    "print(iPortal_data)\n",
    "\n",
    "# list of the names of all products\n",
    "productList = iPortal_data['Product name'].unique()\n",
    "pList = pd.DataFrame(productList)\n",
    "print(pList)\n",
    "\n",
    "# number of unique products \n",
    "len(productList)\n",
    "\n",
    "# data frame that house the count of each device/product \n",
    "productCount = iPortal_data['Product name'].value_counts()\n",
    "\n",
    "\n",
    "# Siemens Scrape\n",
    "\n",
    "#request to get html from website\n",
    "r = requests.get('https://cert-portal.siemens.com/productcert/rss/advisories.atom')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "\n",
    "vulns = soup.find_all('entry')\n",
    "data = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}\n",
    "master_df = pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "##function to get info from each vuln\n",
    "def scrape(vulns):\n",
    "    url = str(vulns).split('<id>', 1)[1]\n",
    "    url = url.split('</id>')[0]\n",
    "    url = url.replace('pdf', 'txt', 2)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    html = str(soup).lower()\n",
    "    \n",
    "    df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}\n",
    "    df = pd.DataFrame(data=df)\n",
    "\n",
    "    #setting values\n",
    "    \n",
    "    #name\n",
    "    name = 'siemens'\n",
    "    \n",
    "    #link \n",
    "    link = url\n",
    "    \n",
    "    #cvss score\n",
    "    if('base score:') in html:\n",
    "        cvss = html.split('base score:', 1)[1]\n",
    "        cvss = cvss.split('summary')[0]\n",
    "        cvss = cvss.strip()\n",
    "    else:\n",
    "        cvss = 'NULL'\n",
    "    \n",
    "    #vector\n",
    "    if('cvss vector') in html:\n",
    "        vector = html.split('cvss vector', 1)[1]\n",
    "        vector = vector.split('/', 1)[1]\n",
    "        vector = vector.split('cwe')[0]\n",
    "        vector = vector.strip()\n",
    "    else:\n",
    "        vector = 'NULL'\n",
    "    \n",
    "    #products\n",
    "    if('affected products and solution') in html:\n",
    "        products = html.split('affected products and solution', 1)[1]\n",
    "        products = products.split('==============================', 1)[1]\n",
    "        products = products.split('workarounds')[0]\n",
    "        products = products.strip()\n",
    "    else:\n",
    "        products = 'NULL'\n",
    "    \n",
    "    data = {'Vendor': [name], 'CVSS Score': [cvss], 'Vector': [vector], 'Products': [products], 'Link': [link]}\n",
    "    temp_df = pd.DataFrame(data=data)\n",
    "    df = df.append(temp_df)\n",
    "    df = df[df.Vendor != '0']\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "for i in range(len(vulns)):\n",
    "    temp_df = scrape(vulns[i])\n",
    "    master_df = master_df.append(temp_df)\n",
    "    \n",
    "\n",
    "siemens_df = master_df \n",
    "\n",
    "# Sick AG Scrape\n",
    "\n",
    "#request to get html from website\n",
    "r = requests.get('https://www.sick.com/de/en/service-and-support/the-sick-product-security-incident-response-team-sick-psirt/w/psirt/#advisories & https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page=9')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "\n",
    "#gets the object that contians information on vulnerabilities\n",
    "advisories = soup.find('tbody')\n",
    "\n",
    "#essentailly breaks up all of the vulnerabilities into their rows and contains them in a list\n",
    "items = advisories.find_all('tr')\n",
    "\n",
    "#initialize dataframe\n",
    "d = {'Vendor': ['0'], 'CVSS Score': ['0'],'Vector': ['0'], 'Products': ['0'], 'PDF': ['0']}\n",
    "sick_vuln_df = pd.DataFrame(data=d)\n",
    "sick_vuln_df\n",
    "\n",
    "#for loop to add info from rows to dataframe, iterate through items\n",
    "length = len(items)\n",
    "index = 1\n",
    "for i in range(length-1):\n",
    "    \n",
    "    #creating variables based on indexed information on the vulnerability\n",
    "    work = items[index]\n",
    "    tds = work.find_all('td')\n",
    "    vendor = 'Sick AG'\n",
    "    CVSS = tds[2].get_text()\n",
    "    Products = tds[3].get_text()\n",
    "    \n",
    "    #PDF url needs to be converted into a string and made into a substring and then appended to base url\n",
    "    PDF = str(tds[5])\n",
    "    string = str(PDF)\n",
    "    update = string.split('href=\"', 1)[1]\n",
    "    final = update.split('\"')[0]\n",
    "    url = 'https://www.sick.com' + final\n",
    "    \n",
    "    #creating data set to create temp dataframe to append to master dataframe of vulnerabilities for SICK\n",
    "    d = {'Vendor': [vendor], 'CVSS Score': [CVSS], 'Vector': 'NULL', 'Products': [Products], 'PDF': [url]}\n",
    "    temp_df = pd.DataFrame(data=d)\n",
    "    sick_vuln_df = sick_vuln_df.append(temp_df)\n",
    "    index = index + 1\n",
    "        \n",
    "sick_vuln_df = sick_vuln_df.reset_index()\n",
    "sick_vuln_df = sick_vuln_df.drop(columns = ['index'])\n",
    "sick_vuln_df = sick_vuln_df.drop([0])\n",
    "\n",
    "# US-CERT Scrape\n",
    "\n",
    "#request to get html from website\n",
    "r = requests.get('https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page=0')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "#function that gets a pages html and then returns a list of all vendors html on the page\n",
    "def vendorsPerPage(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    #getting a list of all vendors html on the page\n",
    "    body = soup.find(role='main')\n",
    "    div = body.find(lambda tag: tag.name == 'div' and tag['class'] == ['view-content'])\n",
    "    item_list = div.find_all('div')\n",
    "    return(item_list)\n",
    "\n",
    "\n",
    "\n",
    "#function that pulls all links of a vendor\n",
    "def vendorLinks(vendor_html):\n",
    "    link_list = vendor_html.find_all('a', href=True)\n",
    "    for i in range(len(link_list)):\n",
    "        update = str(link_list[i]).split('href=\"', 1)[1]\n",
    "        final = update.split('\"')[0]\n",
    "        link_list[i] = final\n",
    "    return(link_list)\n",
    "\n",
    "\n",
    "\n",
    "#function that scrapes data from link list and creates a df for the vendor\n",
    "def vulnScrape(link_list, name):\n",
    "    print(name)\n",
    "    print(link_list)\n",
    "    print(len(link_list))\n",
    "    baseUrl = 'https://www.us-cert.gov'\n",
    "    df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    if len(link_list) >1:\n",
    "        for i in range((len(link_list))):\n",
    "            url = baseUrl + link_list[i]\n",
    "            vuln_df = vulnInfo(url, name)\n",
    "            df = df.append(vuln_df)\n",
    "            \n",
    "    else:\n",
    "        print(link_list)\n",
    "        link_list = str(link_list).split(\"['\", 1)[1]\n",
    "        link_list = link_list.split(\"']\")[0]\n",
    "        url = baseUrl + str(link_list)\n",
    "        vuln_df = vulnInfo(url, name)\n",
    "        df = df.append(vuln_df)\n",
    "\n",
    "    df = df[df.Vendor != '0']        \n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "#function that goes to vuln page and scrapes inf0\n",
    "df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}\n",
    "df = pd.DataFrame(data=df)\n",
    "\n",
    "def vulnInfo(url, name):\n",
    "    print(url)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        \n",
    "    vendor = name\n",
    "    \n",
    "    #parsing cvss score from html\n",
    "    cvss = str(soup)\n",
    "    cvss = cvss.lower()\n",
    "    if('a cvss') in cvss:\n",
    "        cvss = cvss.split('a cvss', 1)[1]\n",
    "        if('score of') in cvss:\n",
    "            cvss = cvss.split('score of ', 1)[1]\n",
    "            cvss = cvss.split(' ')[0]\n",
    "            cvss = cvss[0:3]\n",
    "        else:\n",
    "            cvss = cvss.split('score: ', 1)[1]\n",
    "            cvss = cvss.split(' ')[0]\n",
    "            cvss = cvss[0:3]\n",
    "    else:\n",
    "        cvss = 'NULL'\n",
    "    \n",
    "    #parsing vector from html\n",
    "    vector = str(soup)\n",
    "    vector = vector.lower()\n",
    "    if('cvss vector') in vector:\n",
    "        if('cvss vector string is') in vector:\n",
    "            vector = vector.split('cvss vector string is', 1)[1]\n",
    "            if ((vector.index('(')) == 0) and ((vector.index('a') == 1)):\n",
    "                vector = vector.split('.')[0]\n",
    "            elif ('(cvss:') in vector:\n",
    "                vector = vector.split('/', 1)[1]\n",
    "                vector = vector.split('.')[0]\n",
    "                vector = '(' + vector\n",
    "            elif('>a') in vector:\n",
    "                vector = vector.split('/a', 1)[1]\n",
    "                vector = vector.split('\"')[0]\n",
    "                vector = 'a' + vector\n",
    "                if(')') in vector:\n",
    "                    vector = vector.split(')')[0]\n",
    "            else:\n",
    "                vector = vector.split('>', 1)[1]\n",
    "                vector = vector.split('<')[0]\n",
    "        else:\n",
    "            vector = vector.split('cvss vector string', 1)[1]\n",
    "            vector = vector.split('is', 1)[1]\n",
    "            if ((vector.index('(')) == 0) and ((vector.index('a') == 1)):\n",
    "                vector = vector.split('.')[0]\n",
    "            elif ('(cvss:') in vector:\n",
    "                vector = vector.split('/', 1)[1]\n",
    "                vector = vector.split('.')[0]\n",
    "                vector = '(' + vector\n",
    "            elif('>a') in vector:\n",
    "                vector = vector.split('/a', 1)[1]\n",
    "                vector = vector.split('\"')[0]\n",
    "                vector = 'a' + vector\n",
    "                if(')') in vector:\n",
    "                    vector = vector.split(')')[0]\n",
    "            else:\n",
    "                vector = vector.split('>', 1)[1]\n",
    "                vector = vector.split('<')[0]\n",
    "    else:\n",
    "        vector = 'NULL'\n",
    "    \n",
    "    #parsing products affected from html\n",
    "    products = str(soup)\n",
    "    products = products.lower()\n",
    "    if('affected products') in products:\n",
    "        if('affected:') in products:\n",
    "            print('1')\n",
    "            products = products.split('affected:', 1)[1]\n",
    "            products = products.split('</p>', 1)[1]\n",
    "            if (products.index('u')) == 0:\n",
    "                print('1a')\n",
    "                products = products.split('<li>', 1)[1]\n",
    "                products = products.split('</li>')[0]\n",
    "            elif (products.index('p')) == 0:\n",
    "                print('1b')\n",
    "                print(products)\n",
    "                if('<li>') in products:\n",
    "                    products = products.split('<li>', 1)[1]\n",
    "                    products = products.split('</li>')[0]\n",
    "                elif('<p>') in products:\n",
    "                    products = products.split('<p>', 1)[1]\n",
    "                    products = products.split('</p')[0]\n",
    "            else:\n",
    "                if('<ul>') in products:\n",
    "                    products = products.split('<ul>', 1)[1]\n",
    "                    products = products.split('</ul>')[0]\n",
    "                elif('<p>') in products:\n",
    "                    products = products.split('<p>', 1)[1]\n",
    "                    products = products.split('</p')[0]\n",
    "        elif('affected products') in products:\n",
    "            print('2')\n",
    "            products = products.split('affected products', 1)[1]\n",
    "            products = products.split('/h3', 1)[1]\n",
    "            products = products.split('<h3>')[0]\n",
    "        else:\n",
    "            print('3')\n",
    "            products = products.split('<p>', 1)[1]\n",
    "            products = products.split('</p>')[0]\n",
    "    else:\n",
    "        products = 'NULL'\n",
    "   \n",
    "    #appending variables to df\n",
    "    vuln = {'Vendor': [name], 'CVSS Score': [cvss], 'Vector': [vector], 'Products': [products], 'Link': [url]}\n",
    "    vuln_df = pd.DataFrame(data=vuln)\n",
    "    \n",
    "    #dropping row in df with 0 values\n",
    "    vuln_df = vuln_df.reset_index(drop=True)\n",
    "    return(vuln_df)\n",
    "\n",
    "\n",
    "#iterating through all pages of the website\n",
    "\n",
    "#getting down to the code that holds the url for the last page of results\n",
    "basePgUrl = 'https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page='\n",
    "pg = soup.find_all(role = 'navigation')\n",
    "buttons = pg[1].find_all('li')\n",
    "string = str(buttons[10])\n",
    "update = string.split('href=\"?page=', 1)[1]\n",
    "final = update.split('\"')[0]\n",
    "numbOfPgs = int(final)\n",
    "df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}\n",
    "master_df = pd.DataFrame(data=df)\n",
    "\n",
    "for i in range(numbOfPgs+1):\n",
    "    pgUrl = basePgUrl + str(i) \n",
    "    item_list = vendorsPerPage(pgUrl)\n",
    "    length = len(item_list)\n",
    "    for x in range(length):\n",
    "        print(x)\n",
    "        vName = item_list[x].find('h3')\n",
    "        name = vName.get_text()\n",
    "        name = name.lower() \n",
    "        link_list = vendorLinks(item_list[x])\n",
    "        df = vulnScrape(link_list, name)\n",
    "        master_df = master_df.append(df)\n",
    "        \n",
    "\n",
    "\n",
    "us_cert_df = master_df\n",
    "\n",
    "\n",
    "#appending all dfs together\n",
    "\n",
    "master_df = siemens_df\n",
    "master_df = master_df.append(sick_vuln_df)\n",
    "master_df = master_df.append(us_cert_df)\n",
    "\n",
    "# Exporting everything to excel\n",
    "\n",
    "# export data to new excel\n",
    "writer = pd.ExcelWriter(file_path + 'allData.xlsx')\n",
    "iPortal_data.to_excel(writer, 'IPortal')\n",
    "pList.to_excel(writer, 'List of Products')\n",
    "productCount.to_excel(writer, 'Count per Product')\n",
    "\n",
    "siemens_df = siemens_df[siemens_df.Vendor != '0']\n",
    "siemens_df.to_excel(writer, 'siemensScrape')\n",
    "\n",
    "sick_vuln_df = sick_vuln_df[sick_vuln_df.Vendor != '0']\n",
    "sick_vuln_df.to_excel(writer, 'sickAGScrape')\n",
    "\n",
    "us_cert_df.to_excel(writer, 'US_CertScrape')\n",
    "\n",
    "master_df.to_excel(writer, 'MasterFile')\n",
    "writer.save()\n",
    "\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "writer = pd.ExcelWriter(file_path + 'SickData.xlsx')\n",
    "sick_vuln_df.to_excel(writer, 'Scrape_Data')\n",
    "writer.save()\n",
    "\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "writer = pd.ExcelWriter(file_path + 'Siemensdata.xlsx')\n",
    "siemens_df.to_excel(writer, 'Scrape_Data')\n",
    "writer.save()\n",
    "\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "writer = pd.ExcelWriter(file_path + 'UsCertData.xlsx')\n",
    "us_cert_df.to_excel(writer, 'Scrape_Data')\n",
    "writer.save()\n",
    "\n",
    "# Cleaning and Merging Dfs\n",
    "\n",
    "#importing vulnerability data from excel sheets\n",
    "\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "\n",
    "#SICK Data\n",
    "Sick_Import = pd.read_excel(file_path + 'SickData.xlsx', sheet_name = 'Scrape_Data')\n",
    "Sick_Import = Sick_Import.reset_index()\n",
    "Sick_Import = Sick_Import.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "#Siemens Data\n",
    "Siemens_Import = pd.read_excel(file_path + 'SiemensData.xlsx', sheet_name = 'Scrape_Data')\n",
    "Siemens_Import = Siemens_Import.reset_index()\n",
    "Siemens_Import = Siemens_Import.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "#US Cert Data\n",
    "UsCert_Import = pd.read_excel(file_path + 'UsCertData.xlsx', sheet_name = 'Scrape_Data')\n",
    "UsCert_Import = UsCert_Import.reset_index()\n",
    "UsCert_Import = UsCert_Import.drop(columns = ['index'])\n",
    "\n",
    "#getting vulnerabilities for different vendors from US CERT scrape\n",
    "\n",
    "#gettings Siemens vulnerabilties \n",
    "fSiemens = UsCert_Import[UsCert_Import['Vendor'].str.contains('siemens')]\n",
    "fSiemens['Vendor'] = 'siemens'\n",
    "fSiemens = fSiemens.reset_index()\n",
    "fSiemens = fSiemens.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "#getting Pepperl Fuchs vulnerabilities from US Cert\n",
    "fPepperl = UsCert_Import[UsCert_Import['Vendor'].str.contains('pepperl')]\n",
    "fPepperl['Vendor'] = 'pepperl fuchs'\n",
    "fPepperl = fPepperl.reset_index()\n",
    "PepperlVuln = fPepperl.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "##### can un comment out if Q finishes his part of the code\n",
    "# #getting Pepperl Fuchs vulnerabilities from pepperl scrape\n",
    "# Pepperl_Import = pd.read_excel(file_path + 'PepperlData.xlsx', sheet_name = 'Sheet1')\n",
    "# Pepperl_Import = Pepperl_Import.reset_index()\n",
    "# Pepperl_Import = Pepperl_Import.drop(columns = ['index'])\n",
    "# Pepperl_Import['Vendor'] = 'pepperl fuchs'\n",
    "# Pepperl_Import = Pepperl_Import.apply(lambda x: x.astype(str).str.lower())\n",
    "# Pepperl_Import = Pepperl_Import.drop(columns = ['Unnamed: 4', 'Products'])\n",
    "# Pepperl_Import = Pepperl_Import.rename(columns = {'Links': 'Link'})\n",
    "\n",
    "# #merging two pepperl vulnerability dataframes together\n",
    "# PepperlVuln = PepperlVuln.rename(columns = {'Products': 'Description'})\n",
    "# PepperlVuln = PepperlVuln.append(Pepperl_Import)\n",
    "# PepperlVuln = PepperlVuln.rename(columns = {'Description': 'Products'})\n",
    "\n",
    "\n",
    "#getting sick vulnerabilities\n",
    "fSick = UsCert_Import[UsCert_Import['Vendor'].str.contains('sick')]\n",
    "fSick = fSick.reset_index()\n",
    "fSick = fSick.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "#getting festo vulnerabilities\n",
    "fFesto = UsCert_Import[UsCert_Import['Vendor'].str.contains('fest')]\n",
    "fFesto = fFesto.reset_index()\n",
    "fFesto = fFesto.drop(columns = ['index'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#finalized vulnerabilities dfs\n",
    "\n",
    "#sick\n",
    "SickDf = pd.DataFrame(data=Sick_Import)\n",
    "SickDf = SickDf.rename(columns = {'PDF': 'Link'})\n",
    "SickDf = SickDf.append(fSick)\n",
    "SickDf['Vendor'] = 'sick ag'\n",
    "SickVuln = SickDf\n",
    "SickVuln = SickVuln.drop_duplicates()\n",
    "SickVuln = SickVuln.reset_index()\n",
    "SickVuln = SickVuln.drop(columns = 'index')\n",
    "\n",
    "#siemens\n",
    "SiemensVuln = fSiemens.append(Siemens_Import)\n",
    "SiemensVuln = SiemensVuln.drop_duplicates()\n",
    "SiemensVuln['Vendor'] = 'siemens ag'\n",
    "SiemensVuln = SiemensVuln.reset_index()\n",
    "SiemensVuln = SiemensVuln.drop(columns = 'index')\n",
    "\n",
    "#pepperl fuchs\n",
    "fPepperl = PepperlVuln.drop_duplicates()\n",
    "PepperlVuln = fPepperl\n",
    "\n",
    "#festo\n",
    "fFesto = fFesto.drop_duplicates()\n",
    "FestoVuln = fFesto\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#importing iPortal data from excel\n",
    "\n",
    "#iPortal Data\n",
    "iPortal_data = pd.read_excel(file_path + 'allData.xlsx', sheet_name = 'IPortal')\n",
    "iPortal_Import = iPortal_data.drop(columns = ['Vendor ID'])\n",
    "iPortal_Import = iPortal_Import.drop_duplicates()\n",
    "\n",
    "#cleaning and separating data\n",
    "IPortalDf = pd.DataFrame(data=iPortal_Import)\n",
    "IPortalDf['Product name'] = IPortalDf['Product name'].str.lower()\n",
    "IPortalDf['Online FW version'] = IPortalDf['Online FW version'].str.lower()\n",
    "IPortalDf['Vendor'] = IPortalDf['Vendor'].str.lower()\n",
    "\n",
    "#getting siemens iPortal\n",
    "iPortal_Siemens = iPortal_Import[iPortal_Import['Vendor'].str.contains('siemens')]\n",
    "iPortal_Siemens = iPortal_Siemens.reset_index()\n",
    "iPortal_Siemens = iPortal_Siemens.drop(columns = 'index')\n",
    "\n",
    "#getting sick iPortal\n",
    "iPortal_Sick = iPortal_Import[iPortal_Import['Vendor'].str.contains('sick')]\n",
    "iPortal_Sick = iPortal_Sick.reset_index()\n",
    "iPortal_Sick = iPortal_Sick.drop(columns = 'index')\n",
    "\n",
    "#getting festo iPortal\n",
    "iPortal_Festo = iPortal_Import[iPortal_Import['Vendor'].str.contains('festo')]\n",
    "iPortal_Festo['Vendor'] = 'festo'\n",
    "iPortal_Festo = iPortal_Festo.reset_index()\n",
    "iPortal_Festo = iPortal_Festo.drop(columns = 'index')\n",
    "\n",
    "#getting pepperl iPortal\n",
    "iPortal_Pepperl = iPortal_Import[iPortal_Import['Vendor'].str.contains('pepperl')]\n",
    "iPortal_Pepperl['Vendor'] = 'Pepperl Fuchs'\n",
    "iPortal_Pepperl = iPortal_Pepperl.reset_index()\n",
    "iPortal_Pepperl = iPortal_Pepperl.drop(columns = 'index')\n",
    "\n",
    "\n",
    "\n",
    "def mergeDfsCheck(iPortal, Vuln):\n",
    "\n",
    "    Vuln = Vuln.dropna(subset=['Products'])\n",
    "    Vuln = Vuln.fillna('NaN')\n",
    "    Vuln = Vuln.reset_index()\n",
    "    iPortal = iPortal.dropna()\n",
    "    iPortal = iPortal.fillna('NaN')\n",
    "    iPortal = iPortal.reset_index()\n",
    "    i = 0\n",
    "    data1 = {'Product name': ['0'], 'Firmware version': ['0'], 'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'],\n",
    "             'Description': ['0'], 'Link': ['0']}\n",
    "    FinalDf = pd.DataFrame(data = data1)\n",
    "    for index, row in iPortal.iterrows():\n",
    "        j = 0\n",
    "        substring = iPortal.at[i, 'Product name']\n",
    "        for index, row in Vuln.iterrows():\n",
    "            string = Vuln.at[j, 'Products']\n",
    "            if substring in string:\n",
    "            \n",
    "                print('the Vuln index is:')\n",
    "                print(j)\n",
    "                print('the iPortal index is:')\n",
    "                print(i)\n",
    "                print(substring)\n",
    "            \n",
    "                pName = iPortal.at[i, 'Product name']\n",
    "                print(pName)\n",
    "                FWV = iPortal.at[i, 'Online FW version']\n",
    "                Vendor = iPortal.at[i, 'Vendor']\n",
    "                CVSS = Vuln.at[j, 'CVSS Score']\n",
    "                Vector = Vuln.at[j, 'Vector']\n",
    "                Products = Vuln.at[j, 'Products']\n",
    "                Link = Vuln.at[j, 'Link']\n",
    "            \n",
    "                data2 = {'Vendor': [Vendor], 'Product name': [pName], 'Firmware version': [FWV], 'CVSS Score': [CVSS], \n",
    "                        'Vector': [Vector], 'Description': [Products], 'Link': [Link]}\n",
    "                result = pd.DataFrame(data = data2)\n",
    "\n",
    "                print(result)\n",
    "                FinalDf = FinalDf.append(result) \n",
    "                del result\n",
    "                j = j + 1\n",
    "            \n",
    "            else:\n",
    "                print('no match')      \n",
    "                j = j + 1\n",
    "        i = i + 1\n",
    "\n",
    "    rowCount = FinalDf['Product name'].count()        \n",
    "    if rowCount > 1:\n",
    "        print('Dataframe has been populated')\n",
    "        FinalDf = FinalDf[['Vendor', 'Product name', 'Firmware version', 'CVSS Score', 'Vector', 'Description', 'Link']]\n",
    "        FinalDf = FinalDf.reset_index()\n",
    "        FinalDf = FinalDf.drop(0)\n",
    "        FinalDf = FinalDf.reset_index()\n",
    "        FinalDf = FinalDf.drop(columns = ['index', 'level_0'])\n",
    "    else:\n",
    "        print('Dataframe has not been populated')\n",
    "    \n",
    "\n",
    "    rowCount = FinalDf['Product name'].count()        \n",
    "    if rowCount > 1:\n",
    "        print('Dataframe has been populated')\n",
    "        FinalDf = FinalDf[['Vendor', 'Product name', 'Firmware version', 'CVSS Score', 'Vector', 'Description', 'Link']]\n",
    "        FinalDf = FinalDf.reset_index()\n",
    "        FinalDf = FinalDf.drop(0)\n",
    "        FinalDf = FinalDf.reset_index()\n",
    "        FinalDf = FinalDf.drop(columns = ['index', 'level_0'])\n",
    "    else:\n",
    "        print('Dataframe has not been populated')\n",
    "    return(FinalDf)\n",
    "\n",
    "\n",
    "\n",
    "#running function on all vendor dataframes\n",
    "SiemensF = mergeDfsCheck(iPortal_Siemens, SiemensVuln)\n",
    "SiemensF = SiemensF.drop_duplicates(subset =['Vendor', 'Product name', 'CVSS Score', 'Vector', 'Description', 'Link'], keep='first' )\n",
    "SiemensF = SiemensF.reset_index()\n",
    "SiemesnF = SiemensF.drop(columns = 'index')\n",
    "SiemensF = SiemensF.drop(columns = 'index')\n",
    "FestoF = mergeDfsCheck(iPortal_Festo, FestoVuln)\n",
    "SickF = mergeDfsCheck(iPortal_Sick, SickVuln)\n",
    "PepperlF = mergeDfsCheck(iPortal_Pepperl, PepperlVuln)\n",
    "\n",
    "# data frame that houses the count of each device/product \n",
    "productCount = iPortal_data['Product name'].value_counts()\n",
    "productCount = productCount.reset_index()\n",
    "productCount = productCount.rename(columns = {'Product name': 'Count'})\n",
    "productCount = productCount.rename(columns = {'index': 'Product Name'})\n",
    "productCount['Product Name'] = productCount['Product Name'].str.lower()\n",
    "\n",
    "i = 0\n",
    "for index, row in SiemensF.iterrows():\n",
    "    j = 0;\n",
    "    for index, row in productCount.iterrows():\n",
    "        if SiemensF.at[i, 'Product name'] == productCount.at[j, 'Product Name']:\n",
    "            SiemensF.at[i, 'Product Count'] = productCount.at[j, 'Count'] \n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "#exporting all final dataframes of vulnerabilities\n",
    "\n",
    "file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'\n",
    "writer = pd.ExcelWriter(file_path + 'VulnerabilityData.xlsx')\n",
    "SiemensF.to_excel(writer, 'Siemens Vulnerabilities')\n",
    "FestoF.to_excel(writer, 'Festo Vulnerabilities')\n",
    "SickF.to_excel(writer, 'Sick Vulnerabilities')\n",
    "PepperlF.to_excel(writer, 'Pepperl Fuchs Vulnerabilities')\n",
    "writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
